{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-13T04:10:17.322639Z","iopub.execute_input":"2023-09-13T04:10:17.323417Z","iopub.status.idle":"2023-09-13T04:10:17.690640Z","shell.execute_reply.started":"2023-09-13T04:10:17.323384Z","shell.execute_reply":"2023-09-13T04:10:17.689583Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/vinshee-smritit-text/vinshee_srimit_text-2.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#LEARN HOW TO RELOAD THE MODEL ","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:10:17.692568Z","iopub.execute_input":"2023-09-13T04:10:17.693071Z","iopub.status.idle":"2023-09-13T04:10:17.697692Z","shell.execute_reply.started":"2023-09-13T04:10:17.693035Z","shell.execute_reply":"2023-09-13T04:10:17.696517Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"pip install huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:10:23.217438Z","iopub.execute_input":"2023-09-13T04:10:23.217924Z","iopub.status.idle":"2023-09-13T04:10:38.208958Z","shell.execute_reply.started":"2023-09-13T04:10:23.217840Z","shell.execute_reply":"2023-09-13T04:10:38.207665Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.16.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.12.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2023.9.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.6.3)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2023.7.22)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()\n# key_removed_for_privacy","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:10:38.211213Z","iopub.execute_input":"2023-09-13T04:10:38.211517Z","iopub.status.idle":"2023-09-13T04:10:38.557506Z","shell.execute_reply.started":"2023-09-13T04:10:38.211489Z","shell.execute_reply":"2023-09-13T04:10:38.556528Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48d8edfe2af34f868db5838227bea9b3"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"link https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32","metadata":{}},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:10:46.284218Z","iopub.execute_input":"2023-09-13T04:10:46.284599Z","iopub.status.idle":"2023-09-13T04:11:15.478987Z","shell.execute_reply.started":"2023-09-13T04:10:46.284567Z","shell.execute_reply":"2023-09-13T04:11:15.475695Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:11:15.484807Z","iopub.execute_input":"2023-09-13T04:11:15.485962Z","iopub.status.idle":"2023-09-13T04:11:40.319195Z","shell.execute_reply.started":"2023-09-13T04:11:15.485917Z","shell.execute_reply":"2023-09-13T04:11:40.318104Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"model_name = \"Abhishekdhaka/llama-2-7b-finetuned\"\ndataset_name = \"/kaggle/input/abhay-structured-data/abhay_structured_data.csv\"\n# Fine-tuned model name\nnew_model = \"llama-2-7b-finetuned\"\nlora_r = 64\nlora_alpha = 16\nlora_dropout = 0.1\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bnb_4bit_quant_type = \"nf4\"\nuse_nested_quant = False\noutput_dir = \"./results\"\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_train_epochs = 3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfp16 = False\nbf16 = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"per_device_train_batch_size = 4\nper_device_eval_batch_size = 4\ngradient_accumulation_steps = 1\ngradient_checkpointing = True\n\n\nmax_grad_norm = 0.3\nlearning_rate = 2e-4\nweight_decay = 0.001\noptim = \"paged_adamw_32bit\"\nlr_scheduler_type = \"constant\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_steps = -1\nwarmup_ratio = 0.03\ngroup_by_length = True\nsave_steps = 25\nlogging_steps = 25\nmax_seq_length = None\n\n\npacking = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#device_map = {\"\": 0}\ndevice_map = {\"\": 0, \"optimizer\": 1}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\ndf=pd.read_csv('/kaggle/input/ayushi-vinshee-text-data/ayushi_vinshee_text.csv')\n\ndataset = Dataset.from_pandas(df[['text']])","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:11:40.337088Z","iopub.execute_input":"2023-09-13T04:11:40.337460Z","iopub.status.idle":"2023-09-13T04:11:40.419669Z","shell.execute_reply.started":"2023-09-13T04:11:40.337424Z","shell.execute_reply":"2023-09-13T04:11:40.418730Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df['text'][0]\nprint(len(dataset))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:11:47.025753Z","iopub.execute_input":"2023-09-13T04:11:47.026725Z","iopub.status.idle":"2023-09-13T04:11:47.032595Z","shell.execute_reply.started":"2023-09-13T04:11:47.026690Z","shell.execute_reply":"2023-09-13T04:11:47.031399Z"},"trusted":true},"outputs":[{"name":"stdout","text":"189\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"df.iloc[0]['text']","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:11:50.264718Z","iopub.execute_input":"2023-09-13T04:11:50.265098Z","iopub.status.idle":"2023-09-13T04:11:50.274647Z","shell.execute_reply.started":"2023-09-13T04:11:50.265068Z","shell.execute_reply":"2023-09-13T04:11:50.272968Z"},"trusted":true},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'[INST] If you are a healthcare professional or medical expert, Provide a response emphasizing the importance of consulting a healthcare professional for post-MRI symptoms. [/INST] I just had an MRI on thursday, Ive had many MRIs in the last 10 years but this was my first experience in a 3.0 Tesla imaging unit. I have relapsing remitting multiple sclerosis and am on Tysabri as my disease modifying drug, I have tested positive for the JC virus {papovavirus} and so my MRIs are to watch out for possible signs of PML.  My head has been pounding off and on since thursday and I was wondering if this is common in higher Tesla MRIs. [INST]  It is possible to get headaches after 3.0 Tesla MRI and several of my patients have also complained about that earlier to me. I am not sure of the exact reason, but may be related to noise or higher magnetic field. It settles down in a few days. Wishing you a favorable MRI report. Please get back if you require any additional information [/INST]'"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:11:58.417960Z","iopub.execute_input":"2023-09-13T04:11:58.418319Z","iopub.status.idle":"2023-09-13T04:11:58.425025Z","shell.execute_reply.started":"2023-09-13T04:11:58.418290Z","shell.execute_reply":"2023-09-13T04:11:58.424114Z"},"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text'],\n    num_rows: 189\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"#SETTINGS\n\n\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:12:01.283846Z","iopub.execute_input":"2023-09-13T04:12:01.284255Z","iopub.status.idle":"2023-09-13T04:12:01.293652Z","shell.execute_reply.started":"2023-09-13T04:12:01.284225Z","shell.execute_reply":"2023-09-13T04:12:01.292207Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"#DOWNLOADING THE MODEL\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:12:06.755409Z","iopub.execute_input":"2023-09-13T04:12:06.755841Z","iopub.status.idle":"2023-09-13T04:14:55.389427Z","shell.execute_reply.started":"2023-09-13T04:12:06.755804Z","shell.execute_reply":"2023-09-13T04:14:55.388374Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/632 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d108dfde9794b09b59e3622340ce805"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31e98907ee484fe6a6b6c87cfda3e6ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"548eec229c33421f9dc6116e7db215ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"729af446968b4952bc3e409a91687f92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41fbd441b28f427989a5ebfd8588c23b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2fb8129c7cc48d491fa5d035024463b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/174 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8605880a6b54369b00716311eb7a10e"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:14:55.391567Z","iopub.execute_input":"2023-09-13T04:14:55.391958Z","iopub.status.idle":"2023-09-13T04:14:56.343644Z","shell.execute_reply.started":"2023-09-13T04:14:55.391923Z","shell.execute_reply":"2023-09-13T04:14:56.342638Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/695 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7c58b5745f14408a13ed87a7cb07ed4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec3ce074b9a94b05b7e2e138bf91c8cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb831ab474c344fc924fb21864feb891"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)in/added_tokens.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c5d2cf3b60f4c0bb24162b1cd0d988a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f9cc2ebee4d49d385c1976bc5bbb9f1"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"\n# LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\",\n    #output_dir=\"MEDICAL-LLaMA\",\n    #push_to_hub=True\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:15:10.687146Z","iopub.execute_input":"2023-09-13T04:15:10.687505Z","iopub.status.idle":"2023-09-13T04:15:21.357641Z","shell.execute_reply.started":"2023-09-13T04:15:10.687479Z","shell.execute_reply":"2023-09-13T04:15:21.356680Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e07a1e46dcf4aae8fd13404af90ff0f"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"\ntrainer.train()\n\ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:15:28.137582Z","iopub.execute_input":"2023-09-13T04:15:28.137993Z","iopub.status.idle":"2023-09-13T04:26:21.961727Z","shell.execute_reply.started":"2023-09-13T04:15:28.137961Z","shell.execute_reply":"2023-09-13T04:26:21.960683Z"},"trusted":true},"outputs":[{"name":"stderr","text":"You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='144' max='144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [144/144 10:39, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>2.206600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.088000</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.994900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.950200</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.892100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"RESET THE KERNAL","metadata":{}},{"cell_type":"markdown","source":"# saving model in hub ","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"pip install huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:27:15.511195Z","iopub.execute_input":"2023-09-13T04:27:15.511895Z","iopub.status.idle":"2023-09-13T04:27:28.502083Z","shell.execute_reply.started":"2023-09-13T04:27:15.511844Z","shell.execute_reply":"2023-09-13T04:27:28.500784Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.16.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.12.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2023.9.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.6.3)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2023.7.22)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()\n# key_removed_for_privacy","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:27:28.504640Z","iopub.execute_input":"2023-09-13T04:27:28.505036Z","iopub.status.idle":"2023-09-13T04:27:29.073047Z","shell.execute_reply.started":"2023-09-13T04:27:28.504999Z","shell.execute_reply":"2023-09-13T04:27:29.072237Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f61f33ba44043428d26620b29818288"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:27:37.384531Z","iopub.execute_input":"2023-09-13T04:27:37.384932Z","iopub.status.idle":"2023-09-13T04:27:49.567931Z","shell.execute_reply.started":"2023-09-13T04:27:37.384887Z","shell.execute_reply":"2023-09-13T04:27:49.566551Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:27:49.570593Z","iopub.execute_input":"2023-09-13T04:27:49.571011Z","iopub.status.idle":"2023-09-13T04:28:06.392442Z","shell.execute_reply.started":"2023-09-13T04:27:49.570971Z","shell.execute_reply":"2023-09-13T04:28:06.391394Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:28:19.615451Z","iopub.execute_input":"2023-09-13T04:28:19.615829Z","iopub.status.idle":"2023-09-13T04:30:12.805021Z","shell.execute_reply.started":"2023-09-13T04:28:19.615798Z","shell.execute_reply":"2023-09-13T04:30:12.803949Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95aa622299204981893fbee8a1f02432"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"model.push_to_hub('Abhishekdhaka/llama2vinsheeandayushi')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:36:07.893685Z","iopub.execute_input":"2023-09-13T04:36:07.894202Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32ceeaec6a4144f3a4a00d59fae402ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1c4133f65e4456d8e5755006b970d89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1f653c083964424b7731fb49a7617de"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"tokenizer.push_to_hub('Abhishekdhaka/llama2vinsheeandayushi')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"name_model='Abhishekdhaka/llama2vinsheeandayushi'\nmodel.push_to_hub(name_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T04:35:24.126975Z","iopub.execute_input":"2023-09-13T04:35:24.127445Z","iopub.status.idle":"2023-09-13T04:35:24.248228Z","shell.execute_reply.started":"2023-09-13T04:35:24.127406Z","shell.execute_reply":"2023-09-13T04:35:24.246783Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m name_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbhishekdhaka/llama2vinsheeandayushi\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_temp_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpush_to_hub(new_model, use_temp_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:811\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[0;34m(self, repo_id, use_temp_dir, commit_message, private, use_auth_token, max_shard_size, create_pr, safe_serialization, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m     use_temp_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(working_dir)\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m working_or_temp_dir(working_dir\u001b[38;5;241m=\u001b[39mworking_dir, use_temp_dir\u001b[38;5;241m=\u001b[39muse_temp_dir) \u001b[38;5;28;01mas\u001b[39;00m work_dir:\n\u001b[0;32m--> 811\u001b[0m     files_timestamps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_files_timestamps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwork_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;66;03m# Save all files.\u001b[39;00m\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_pretrained(work_dir, max_shard_size\u001b[38;5;241m=\u001b[39mmax_shard_size, safe_serialization\u001b[38;5;241m=\u001b[39msafe_serialization)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:672\u001b[0m, in \u001b[0;36mPushToHubMixin._get_files_timestamps\u001b[0;34m(self, working_dir)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_files_timestamps\u001b[39m(\u001b[38;5;28mself\u001b[39m, working_dir: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike]):\n\u001b[1;32m    669\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;124;03m    Returns the list of files with their last modification timestamp.\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {f: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetmtime(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(working_dir, f)) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworking_dir\u001b[49m\u001b[43m)\u001b[49m}\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'llama2vinsheeandayushi'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'llama2vinsheeandayushi'","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## sementic search","metadata":{}},{"cell_type":"code","source":"!pip install cohere umap-learn altair annoy datasets tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport cohere\nimport numpy as np\nimport re\nimport pandas as pd\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nimport umap\nimport altair as alt\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom annoy import AnnoyIndex\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_colwidth', None)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = load_dataset(\"trec\", split=\"train\")\ndf = pd.DataFrame(dataset)[:1000]\ndf.head(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\napi_key = ''\n\nco = cohere.Client(api_key)\nembeds = co.embed(texts=list(df['text']),\n                  model='embed-english-v2.0').embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"search_index = AnnoyIndex(np.array(embeds).shape[1], 'angular')\n\nfor i in range(len(embeds)):\n    search_index.add_item(i, embeds[i])\nsearch_index.build(10) # 10 trees\nsearch_index.save('test.ann')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = f\"Question:'{df.iloc[example_id]['text']}'\\nNearest neighbors:\"\nquery_embed = co.embed(texts=[query],\n                  model=\"embed-english-v2.0\").embeddings\n\nsimilar_item_ids = search_index.get_nns_by_vector(query_embed[0],10,\n                                                include_distances=True)\nresults = pd.DataFrame(data={'texts': df.iloc[similar_item_ids[0]]['text'], \n                             'distance': similar_item_ids[1]})\n\n\nprint(f\"Query:'{query}'\\nNearest neighbors:\")\nresults","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Summarization ************","metadata":{}},{"cell_type":"code","source":"$ pip install cohere","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cohere\nco = cohere.Client(api_key)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text =\"\"\"It's an exciting day for the development community. Cohere's state-of-the-art language AI is now available through Amazon SageMaker. This makes it easier for developers to deploy Cohere's pre-trained generation language model to Amazon SageMaker, an end-to-end machine learning (ML) service. Developers, data scientists, and business analysts use Amazon SageMaker to build, train, and deploy ML models quickly and easily using its fully managed infrastructure, tools, and workflows.\nAt Cohere, the focus is on language. The company's mission is to enable developers and businesses to add language AI to their technology stack and build game-changing applications with it. Cohere helps developers and businesses automate a wide range of tasks, such as copywriting, named entity recognition, paraphrasing, text summarization, and classification. The company builds and continually improves its general-purpose large language models (LLMs), making them accessible via a simple-to-use platform. Companies can use the models out of the box or tailor them to their particular needs using their own custom data.\nDevelopers using SageMaker will have access to Cohere's Medium generation language model. The Medium generation model excels at tasks that require fast responses, such as question answering, copywriting, or paraphrasing. The Medium model is deployed in containers that enable low-latency inference on a diverse set of hardware accelerators available on AWS, providing different cost and performance advantages for SageMaker customers.\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"response = co.summarize(\n    text=text,\n    model='command',\n    length='medium',\n    extractiveness='medium'\n)\n\nsummary = response.summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CoRank ","metadata":{}},{"cell_type":"code","source":"$ pip install cohere","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cohere\nco = cohere.Client('key')\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"we will get the docs for nns","metadata":{}},{"cell_type":"code","source":"query = \"What is the capital of the United States?\"\ndocs = [\n    \"Carson City is the capital city of the American state of Nevada. At the 2010 United States Census, Carson City had a population of 55,274.\",\n    \"The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean that are a political division controlled by the United States. Its capital is Saipan.\",\n    \"Charlotte Amalie is the capital and largest city of the United States Virgin Islands. It has about 20,000 people. The city is on the island of Saint Thomas.\",\n    \"Washington, D.C. (also known as simply Washington or D.C., and officially as the District of Columbia) is the capital of the United States. It is a federal district. The President of the USA and many major national government offices are in the territory. This makes it the political center of the United States of America.\",\n    \"Capital punishment (the death penalty) has existed in the United States since before the United States was a country. As of 2017, capital punishment is legal in 30 of the 50 states. The federal government (including the United States military) also uses capital punishment.\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = co.rerank(query=query, documents=docs, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\nfor idx, r in enumerate(results):\n  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n  print(f\"Document: {r.document['text']}\")\n  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n  print(\"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## gradio","metadata":{}},{"cell_type":"code","source":"!pip install text_generation","metadata":{"execution":{"iopub.status.busy":"2023-09-12T06:04:32.508761Z","iopub.execute_input":"2023-09-12T06:04:32.509438Z","iopub.status.idle":"2023-09-12T06:04:47.440052Z","shell.execute_reply.started":"2023-09-12T06:04:32.509400Z","shell.execute_reply":"2023-09-12T06:04:47.438865Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting text_generation\n  Downloading text_generation-0.6.0-py3-none-any.whl (10 kB)\nRequirement already satisfied: aiohttp<4.0,>=3.8 in /opt/conda/lib/python3.10/site-packages (from text_generation) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from text_generation) (0.16.4)\nRequirement already satisfied: pydantic<2.0,>=1.10 in /opt/conda/lib/python3.10/site-packages (from text_generation) (1.10.10)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->text_generation) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->text_generation) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->text_generation) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->text_generation) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->text_generation) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->text_generation) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->text_generation) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (3.12.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (2023.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (4.65.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (4.6.3)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.12->text_generation) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub<1.0,>=0.12->text_generation) (3.0.9)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0,>=3.8->text_generation) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.12->text_generation) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.12->text_generation) (2023.5.7)\nInstalling collected packages: text_generation\nSuccessfully installed text_generation-0.6.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install gradio","metadata":{"execution":{"iopub.status.busy":"2023-09-12T06:04:47.444311Z","iopub.execute_input":"2023-09-12T06:04:47.444616Z","iopub.status.idle":"2023-09-12T06:05:04.470884Z","shell.execute_reply.started":"2023-09-12T06:04:47.444587Z","shell.execute_reply":"2023-09-12T06:05:04.469595Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting gradio\n  Downloading gradio-3.43.2-py3-none-any.whl (20.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (22.1.0)\nRequirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (5.0.1)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio) (0.98.0)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting gradio-client==0.5.0 (from gradio)\n  Downloading gradio_client-0.5.0-py3-none-any.whl (298 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting httpx (from gradio)\n  Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface-hub>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.16.4)\nRequirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (5.12.0)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.2)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.3)\nRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.7.1)\nRequirement already satisfied: numpy~=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.23.5)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.9.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (21.3)\nRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.5.3)\nRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (9.5.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.10.10)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\nCollecting python-multipart (from gradio)\n  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0)\nRequirement already satisfied: requests~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.31.0)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.6.3)\nRequirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.22.0)\nRequirement already satisfied: websockets<12.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (11.0.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==0.5.0->gradio) (2023.6.0)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.17.3)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.14.0->gradio) (3.12.2)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.14.0->gradio) (4.65.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.4)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio) (2023.5.7)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (8.1.3)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio) (0.27.0)\nCollecting httpcore<0.19.0,>=0.18.0 (from httpx->gradio)\n  Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio) (1.3.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from httpcore<0.19.0,>=0.18.0->httpx->gradio) (3.7.0)\nRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.19.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->httpcore<0.19.0,>=0.18.0->httpx->gradio) (1.1.1)\nBuilding wheels for collected packages: ffmpy\n  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5596 sha256=7131ed00480352b4c60b96b6ed9a011f0d439a78310d5d8ee9f822982937fa06\n  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\nSuccessfully built ffmpy\nInstalling collected packages: ffmpy, semantic-version, python-multipart, httpcore, httpx, gradio-client, gradio\nSuccessfully installed ffmpy-0.3.1 gradio-3.43.2 gradio-client-0.5.0 httpcore-0.18.0 httpx-0.25.0 python-multipart-0.0.6 semantic-version-2.10.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport io\nimport IPython.display\nfrom PIL import Image\nimport base64 \nimport requests \nrequests.adapters.DEFAULT_TIMEOUT = 60\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\nhf_api_key = 'key'","metadata":{"execution":{"iopub.status.busy":"2023-09-12T06:05:04.472923Z","iopub.execute_input":"2023-09-12T06:05:04.474013Z","iopub.status.idle":"2023-09-12T06:05:04.495719Z","shell.execute_reply.started":"2023-09-12T06:05:04.473973Z","shell.execute_reply":"2023-09-12T06:05:04.494838Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=gr.Textbox(lines=2, placeholder=\"Name Here...\"),\n    outputs=\"text\",\n)\ndemo.launch()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:04:52.683962Z","iopub.execute_input":"2023-09-12T07:04:52.684608Z","iopub.status.idle":"2023-09-12T07:04:53.242849Z","shell.execute_reply.started":"2023-09-12T07:04:52.684571Z","shell.execute_reply":"2023-09-12T07:04:53.240395Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreet\u001b[39m(name):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'"],"ename":"ModuleNotFoundError","evalue":"No module named 'gradio'","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"the library provide an option for image uplaod also","metadata":{}},{"cell_type":"markdown","source":"history: a list of list representing the conversations up until that point. Each inner list consists of two str representing a pair: [user input, bot response].\n","metadata":{}},{"cell_type":"code","source":"import random\nimport gradio as gr\n\ndef random_response(message, history):\n    return random.choice([\"Yes\", \"No\"])\n\ndemo = gr.ChatInterface(random_response)\n\ndemo.launch(share='True')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T06:33:47.332894Z","iopub.execute_input":"2023-09-12T06:33:47.333642Z","iopub.status.idle":"2023-09-12T06:33:50.754019Z","shell.execute_reply.started":"2023-09-12T06:33:47.333604Z","shell.execute_reply":"2023-09-12T06:33:50.753053Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Running on local URL:  http://127.0.0.1:7862\nRunning on public URL: https://3ed3f5709a2c3ca545.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://3ed3f5709a2c3ca545.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"this is a bit more complex structure shows that you move around the things ","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import gradio as gr\n\n\n# def flip_text(x):\n#     return x[::-1]\n\n\n# def flip_image(x):\n#     return np.fliplr(x)\n\n\n# with gr.Blocks() as demo:\n#     gr.Markdown(\"Flip text or image files using this demo.\")\n#     with gr.Tab(\"Flip Text\"):\n#         text_input = gr.Textbox()\n#         text_output = gr.Textbox()\n#         text_button = gr.Button(\"Flip\")\n#     with gr.Tab(\"Flip Image\"):\n#         with gr.Row():\n#             image_input = gr.Image()\n#             image_output = gr.Image()\n#         image_button = gr.Button(\"Flip\")\n\n#     with gr.Accordion(\"Open for More!\"):\n#         gr.Markdown(\"Look at me...\")\n\n#     text_button.click(flip_text, inputs=text_input, outputs=text_output)\n#     image_button.click(flip_image, inputs=image_input, outputs=image_output)\n\n# demo.launch()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is the best user interface which got from course","metadata":{}},{"cell_type":"code","source":"def format_chat_prompt(message, chat_history, instruction):\n    prompt = f\"System:{instruction}\"\n    for turn in chat_history:\n        user_message, bot_message = turn\n        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n    return prompt\n\ndef respond(message, chat_history, instruction, temperature=0.7):\n    prompt = format_chat_prompt(message, chat_history, instruction)\n    chat_history = chat_history + [[message, \"\"]]\n    stream = client.generate_stream(prompt,\n                                      max_new_tokens=1024,\n                                      stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"],\n                                      temperature=temperature)\n                                      \n    acc_text = \"\"\n\n    for idx, response in enumerate(stream):\n            text_token = response.token.text\n\n            if response.details:\n                return\n\n            if idx == 0 and text_token.startswith(\" \"):\n                text_token = text_token[1:]\n\n            acc_text += text_token\n            last_turn = list(chat_history.pop(-1))\n            last_turn[-1] += acc_text\n            chat_history = chat_history + [last_turn]\n            yield \"\", chat_history\n            acc_text = \"\"\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n    msg = gr.Textbox(label=\"Prompt\")\n    with gr.Accordion(label=\"Advanced options\",open=False):\n        system = gr.Textbox(label=\"System message\", lines=2, value=\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\")\n        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1, value=0.7, step=0.1)\n    btn = gr.Button(\"Submit\")\n    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n\n    btn.click(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])\n    msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot]) #Press enter to submit\ngr.close_all()\n#demo.queue().launch(share=True, server_port=int(os.environ['PORT4']))\ndemo.launch()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T06:35:39.846503Z","iopub.execute_input":"2023-09-12T06:35:39.846983Z","iopub.status.idle":"2023-09-12T06:35:44.923705Z","shell.execute_reply.started":"2023-09-12T06:35:39.846943Z","shell.execute_reply":"2023-09-12T06:35:44.922794Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Closing server running on port: 7860\nRunning on local URL:  http://127.0.0.1:7860\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\nRunning on public URL: https://0dc9099d783b5a4b69.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://0dc9099d783b5a4b69.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"gr.close_all()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# database","metadata":{}},{"cell_type":"code","source":"import sqlite3\n#when you do this the database is automatically created in output \nconn = sqlite3.connect('your_database_new.db')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cursor = conn.cursor()\n\n# Create a users table\ncursor.execute('''\n    CREATE TABLE IF NOT EXISTS users (\n        id INTEGER PRIMARY KEY,\n        username TEXT UNIQUE NOT NULL,\n        password_hash TEXT NOT NULL\n    )\n''')\n\n# Commit the changes\nconn.commit()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Insert data into the users table\ncursor.execute(\"INSERT INTO users (username, password_hash) VALUES (?, ?)\", ('example_user', 'hashed_password'))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conn.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport shutil\ndatabase_source = 'your_database.db'\ndestination_directory = '/kaggle/working/'\n\nshutil.copy(database_source, destination_directory)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"res = cursor.execute(\"SELECT * FROM users \")\nres.fetchall()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## log in ","metadata":{}},{"cell_type":"code","source":"!pip install Flask\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install Flask-Login\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from flask import Flask, render_template, request, redirect, url_for, session\napp = Flask(__name__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\napp.secret_key = os.urandom(24)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from flask_login import LoginManager, UserMixin, login_user, login_required, logout_user\n\nlogin_manager = LoginManager()\nlogin_manager.init_app(app)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class User(UserMixin):\n    def __init__(self, id):\n        self.id = id\nusers = {'user1': {'password': 'password1'}}\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    if request.method == 'POST':\n        username = request.form['username']\n        password = request.form['password']\n        if username in users and users[username]['password'] == password:\n            user = User(username)\n            login_user(user)\n            return redirect(url_for('dashboard'))\n        else:\n            return 'Invalid login credentials. Please try again.'\n    return render_template('login.html')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@app.route('/dashboard')\n@login_required\ndef dashboard():\n    return 'Welcome to the dashboard, {}'.format(current_user.id)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@app.route('/logout')\n@login_required\ndef logout():\n    logout_user()\n    return 'You have been logged out. Goodbye!'\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#flask is not working \napp.run()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## log in from gradio ","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport gradio as gr\n\n# Mock user data (replace with your actual user data from your SQLite database)\nusers = {'user1': {'password': 'password1'}}\n\n# Define a User model for login\nclass User:\n    def __init__(self, username):\n        self.username = username\n\n    def authenticate(self, password):\n        return self.username in users and users[self.username]['password'] == password\n\n# Your existing functions for data analysis (get_count_ride_type, get_most_popular_stations)\n# ...\n\n# Gradio setup\nwith gr.Blocks() as demo:\n    with gr.Row():\n        bike_type = gr.Plot()\n        station = gr.Plot()\n\n    demo.load(get_count_ride_type, inputs=None, outputs=bike_type)\n    demo.load(get_most_popular_stations, inputs=None, outputs=station)\n\n# Define a custom interface for user login\ndef login_interface():\n    username = gr.Textbox(label=\"Username\")\n    password = gr.Textbox(label=\"Password\", type=\"password\")\n    submit = gr.Button(label=\"Log In\")\n\n    return gr.Interface(\n        fn=lambda username, password: User(username).authenticate(password),\n        inputs=[username, password],\n        outputs=gr.Label(),\n        live=True,\n        examples=[[\"user1\", \"password1\"]],\n        layout=\"vertical\",\n    )\n\n# Run the Gradio application\nlogin_app = login_interface()\nlogin_app.launch()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install psycopg2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nDB_USER = os.getenv(\"DB_USER\")\nDB_USER='/kaggle/working/your_database.db'\n\nconnection_string = f\"postgresql://{DB_USER}\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef get_count_ride_type():\n    df = pd.read_sql(\n    \"\"\"\n        SELECT *\n        FROM users\n\n    \"\"\",\n    con=connection_string\n    )\n    fig_m, ax = plt.subplots()\n    ax.bar(x=df['rideable_type'], height=df['n'])\n    ax.set_title(\"Number of rides by bycycle type\")\n    ax.set_ylabel(\"Number of Rides\")\n    ax.set_xlabel(\"Bicycle Type\")\n    return fig_m\n\n\ndef get_most_popular_stations():\n\n    df = pd.read_sql(\n        \"\"\"\n    SELECT COUNT(ride_id) as n, MAX(start_station_name) as station\n    FROM RIDES\n    WHERE start_station_name is NOT NULL\n    GROUP BY start_station_id\n    ORDER BY n DESC\n    LIMIT 5\n    \"\"\",\n    con=connection_string\n    )\n    fig_m, ax = plt.subplots()\n    ax.bar(x=df['station'], height=df['n'])\n    ax.set_title(\"Most popular stations\")\n    ax.set_ylabel(\"Number of Rides\")\n    ax.set_xlabel(\"Station Name\")\n    ax.set_xticklabels(\n        df['station'], rotation=45, ha=\"right\", rotation_mode=\"anchor\"\n    )\n    ax.tick_params(axis=\"x\", labelsize=8)\n    fig_m.tight_layout()\n    return fig_m","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def method_here():\n    print('hi you are welcome')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\n\nwith gr.Blocks() as demo:\n    #with gr.Row():\n\n    demo.load(method_here, inputs=None, outputs=None)\n    #demo.load(get_count_ride_type, inputs=None, outputs=None)\n    \n\ndemo.launch()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#THIS IS BASIC LOG IN PAGE\nimport gradio as gr\n\n# Mock user data (replace with your actual user data from your database)\nusers = {'user1': {'password': 'password1'}, 'user2': {'password': 'password2'}}\n\n# Function to check user credentials\ndef authenticate(username, password):\n    if username in users and users[username]['password'] == password:\n        return f\"Hi, {username}!\"\n    else:\n        return \"Invalid credentials. Please try again.\"\n\n# Gradio interface\ninterface = gr.Interface(\n    fn=authenticate,\n    inputs=[\n        gr.Textbox(label=\"Username\"),\n        gr.Textbox(label=\"Password\", type=\"password\")\n    ],\n    outputs=gr.Textbox(default=\"\"),\n    live=True,\n    title=\"User Login\",\n    description=\"Enter your username and password.\",\n)\n\n# Launch the Gradio interface\ninterface.launch()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Experimental : for user login ","metadata":{}},{"cell_type":"code","source":"import gradio as gr\n\n\nusers = {'user1': {'password': 'password1'}, 'user2': {'password': 'password2'}}\n\n\ndef chatbot(input_text):\n\n    response = \"Chatbot: You said, '{}'. Please replace this with your chatbot's response.\".format(input_text)\n    return response\n\n\ndef authenticate(username, password):\n    if username in users and users[username]['password'] == password:\n        return True\n    else:\n        return False\n\n\nchatbot_interface = gr.Interface(\n    fn=chatbot,\n    inputs=gr.Textbox(label=\"You:\"),\n    outputs=gr.Textbox(label=\"Chatbot:\"),\n    live=True,\n    title=\"Chatbot\",\n    description=\"Enter your messages to chat with the bot.\",\n)\n\n\nlogin_interface = gr.Interface(\n    fn=authenticate,\n    inputs=[\n        gr.Textbox(label=\"Username\"),\n        gr.Textbox(label=\"Password\", type=\"password\")\n    ],\n    outputs=gr.Textbox(default=\"enter your password\"),\n    live=True,\n    title=\"User Login\",\n    description=\"Enter your username and password.\",\n)\n\n\nlogin_interface.launch()\n\n\nauthenticated = False\n\n\nwhile not authenticated:\n    if login_interface.get_latest_output() == \"True\":\n        authenticated = True\n\nchatbot_interface.launch()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport gradio as gr\n\n\nusers = {'user1': {'password': 'password1'}, 'user2': {'password': 'password2'}}\n\ndef chatbot(input_text):\n\n    response = \"Chatbot: You said, '{}'. Please replace this with your chatbot's response.\".format(input_text)\n    \n    return response\n\n\ndef authenticate(username, password):\n    if username in users and users[username]['password'] == password:\n        chatbot_interface.launch()\n        demo.close()\n        chatbot_interface.close()\n        return \"welcome\"\n    else:\n        return \"wrong credentials\"\n\n\n\nchatbot_interface = gr.Interface(\n    fn=chatbot,\n    inputs=gr.Textbox(label=\"You:\"),\n    outputs=gr.Textbox(label=\"Chatbot:\"),\n    live=True,\n    title=\"Chatbot\",\n    description=\"Enter your messages to chat with the bot.\",\n)\nwith gr.Blocks() as demo:\n    #chatbot = gr.Chatbot(height=240) #just to fit the notebook\n    user_name = gr.Textbox(label=\"User Name\")\n    user_password = gr.Textbox(label=\"User Password\")\n#     with gr.Accordion(label=\"Advanced options\",open=False):\n#         system = gr.Textbox(label=\"System message\", lines=2, value=\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\")\n#         temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1, value=0.7, step=0.1)\n    btn = gr.Button(\"Login\")\n    clear = gr.ClearButton(components=[user_name, user_password], value=\"Reset\")\n    correct_id = gr.Textbox(label=\"Correct ID\")\n    btn.click(authenticate, inputs=[user_name, user_password], outputs=[correct_id])\n    print(correct_id)\n    #msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot]) #Press enter to submit\ngr.close_all()\n#demo.queue().launch(share=True, server_port=int(os.environ['PORT4']))\ndemo.launch()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#my try method 2\nimport gradio as gr\n\n\nusers = {'user1': {'password': 'password1'}, 'user2': {'password': 'password2'}}\n\n\ndef chatbot(input_text):\n\n    response = \"Chatbot: You said, '{}'. Please replace this with your chatbot's response.\".format(input_text)\n    return response\n\n\ndef authenticate(username, password):\n    if username in users and users[username]['password'] == password:\n        chatbot_interface.launch()\n        return \"welcome\"\n    else:\n        return \"wrong credentials\"\n\n\n    \nlogin_interface = gr.Interface(\n    fn=authenticate,\n    inputs=gr.Textbox(label=\"username:\"),\n    outputs=gr.Textbox(label=\"password:\"),\n    live=True,\n    title=\"Chatbot\",\n    description=\"Enter your messages to chat with the bot.\",\n)\n\nchatbot_interface = gr.Interface(\n    fn=chatbot,\n    inputs=gr.Textbox(label=\"You:\"),\n    outputs=gr.Textbox(label=\"Chatbot:\"),\n    live=True,\n    title=\"Chatbot\",\n    description=\"Enter your messages to chat with the bot.\",\n)\nwith gr.Blocks() as demo:\n    #chatbot = gr.Chatbot(height=240) #just to fit the notebook\n    user_name = gr.Textbox(label=\"User Name\")\n    user_password = gr.Textbox(label=\"User Password\")\n#     with gr.Accordion(label=\"Advanced options\",open=False):\n#         system = gr.Textbox(label=\"System message\", lines=2, value=\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\")\n#         temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1, value=0.7, step=0.1)\n    btn = gr.Button(\"Login\")\n    clear = gr.ClearButton(components=[user_name, user_password], value=\"Reset\")\n    correct_id = gr.Textbox(label=\"Correct ID\")\n    btn.click(authenticate, inputs=[user_name, user_password], outputs=[correct_id])\n    print(correct_id)\n    #msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot]) #Press enter to submit\ngr.close_all()\n#demo.queue().launch(share=True, server_port=int(os.environ['PORT4']))\ndemo.launch()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\n\n\ndef process_inputs(input1, input2, input3):\n\n    result = f\"Input 1: {input1}\\nInput 2: {input2}\\nInput 3: {input3}\"\n    \n    \n    iface2.launch()\n    return result\n\ndef process_inputss(input1, input2, input3):\n    iface.close()\n\n    result = f\"Input 11111: {input1}\\nInput 2: {input2}\\nInput 3: {input3}\"\n\n    return result\n\n\ninput1 = gr.inputs.Textbox(label=\"Input 1\")\ninput2 = gr.inputs.Textbox(label=\"Input 2\")\ninput3 = gr.inputs.Textbox(label=\"Input 3\")\n\n\niface = gr.Interface(\n    fn=process_inputs,\n    inputs=[input1, input2, input3],\n    outputs=\"text\"\n)\niface2 = gr.Interface(\n    fn=process_inputss,\n    inputs=[input1, input2, input3],\n    outputs=\"text\"\n)\n\n# Launch the Gradio interface\niface.launch()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\nimport threading\n\n\nvalid_username = \"user123\"\nvalid_password = \"password123\"\n\n\ndef check_credentials(username, password):\n    return username == valid_username and password == valid_password\nusername = gr.inputs.Textbox(label=\"Input 1\")\npassword = gr.inputs.Textbox(label=\"Input 2\")\n\n\ndef login_interface(username, password):\n    if check_credentials(username, password):\n\n        chat_thread = threading.Thread(target=chat_interface.launch())\n        chat_thread.start()\n\n\n        login_interface.close()\n\n        return \"Login successful. Redirecting to the chat interface...\"\n    else:\n        return \"Invalid username or password. Please try again.\"\n\n\ndef chat_interface(input_text):\n    # Implement your chatbot logic here\n    response = \"Chatbot response: You said - \" + input_text\n    return response\n\n\nlogin_interface = gr.Interface(fn=login_interface, inputs=[username, password], outputs=None, live=False)\nchatbot_interface = gr.Interface(fn=chat_interface, inputs=\"text\", outputs=\"text\", live=True)\n\nif __name__ == \"__main__\":\n\n    login_thread = threading.Thread(target=login_interface.launch)\n    login_thread.start()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#if you want to upload the training report NOTE: it will not upload the saved model you have to save the model locally\ntrainer.push_to_hub()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#since we are using 4bit so this code will not work \n#model.push_to_hub('Abhishekdhaka/MEDICAL-LLaMA')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = \"मुझे इस दर्द के लिए कौन सी दवाई सबसे अच्छी है?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\noutput_directory = '/kaggle/working/model_saving'\nos.makedirs(output_directory, exist_ok=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import HfApi\napi = HfApi()\nfrom huggingface_hub import create_repo,delete_repo\n#delete_repo(repo_id=\"Abhishekdhaka/MedicalLLaMA_data\")\ncreate_repo(\"Abhishekdhaka/MedicalLLaMA_data\", repo_type=\"space\",space_sdk='docker')\n\n#To delete a resposity\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\napi.upload_folder(\n    folder_path=\"/kaggle/working/results\",\n    repo_id=\"Abhishekdhaka/MedicalLLaMA_data\",\n    repo_type=\"space\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# to download the complete repo back you can use this code ##IT IS NOT WORKING AS OF NOW\nfrom huggingface_hub import snapshot_download\nsnapshot_download(repo_id=\"Abhishekdhaka/MedicalLLaMA_data\", repo_type=\"space\")\n\nsnapshot_download(repo_id=\"google/fleurs\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}